# Structure of the Project 
```text
.                                                       # Root project directory
â”œâ”€â”€ Final_Outputs                                       # Screenshots of each step for documentation/evidence
â”‚   â”œâ”€â”€ AFTER DATA UPLOADED BY USER BACKEND( Step 12)   # Backend validation after user uploads data
â”‚   â”œâ”€â”€ Airflow (Step 13)                               # Airflow UI screenshots after DAG run
â”‚   â”œâ”€â”€ CRAWLER(Step 5)                                 # Screenshots of the web crawler setup
â”‚   â”œâ”€â”€ Dirft logs (Step 14)                            # Screenshots of drift report logs from Airflow
â”‚   â”œâ”€â”€ EC2 (MLFLOW) [STEP 8]                           # EC2 instance setup for MLflow
â”‚   â”œâ”€â”€ Final Flask Code (Step 13)                      # Flask backend UI and API screenshots
â”‚   â”œâ”€â”€ Glue connections (Step 6)                       # AWS Glue crawler and connection setup
â”‚   â”œâ”€â”€ IAM                                             # IAM roles and permissions
â”‚   â”œâ”€â”€ Inital S3 with Lead  Data(Step 1)               # Initial data upload to S3
â”‚   â”œâ”€â”€ MLflow (Step 10)                                # Screenshots of MLflow model tracking
â”‚   â”œâ”€â”€ REDSHIFT (Step 4)                               # Amazon Redshift setup and tables
â”‚   â”œâ”€â”€ S3 - REDSHIFT (Step 7)                          # S3 to Redshift connection proof
â”‚   â”œâ”€â”€ S3 Buckets Final State with all files(Step 15)  # Final structure of S3 with all files
â”‚   â”œâ”€â”€ Security Groups (Step 2)                        # Security Group configuration screenshots
â”‚   â”œâ”€â”€ Training (Step 9)                               # Model training pipeline run on EC2
â”‚   â”œâ”€â”€ User uploads Data(Step 11)                      # Frontend where user uploads CSV
â”‚   â””â”€â”€ Vpc Endpoints  (Step 3)                         # VPC endpoints setup for private access
â”œâ”€â”€ README.md                                           # Project instructions and documentation
â”œâ”€â”€ data                                                # Local data used for EDA and modeling
â”‚   â”œâ”€â”€ eda_report.html                                 # Auto-generated EDA report
â”‚   â”œâ”€â”€ lead_scoring.csv                                # Raw training dataset
â”‚   â””â”€â”€ test_lead_scoring.csv                           # Test dataset for validation
â”œâ”€â”€ lead_data_schema.txt                                # Schema of the incoming data file
â”œâ”€â”€ models                                              # Pickled models and preprocessing pipelines
â”‚   â”œâ”€â”€ full_pipeline.pkl                               # Pipeline with preprocessing steps
â”‚   â”œâ”€â”€ full_pipeline_with_model.pkl                    # Full pipeline including model
â”‚   â””â”€â”€ preprocessor.pkl                                # Only preprocessing object
â”œâ”€â”€ reports
â”‚   â””â”€â”€ drift                                           # Data drift reports (HTML + JSON)
â”‚       â”œâ”€â”€ drift_lead_data_vs_uploaded_*.html/json     # Drift between base data and uploaded
â”‚       â””â”€â”€ drift_train_vs_test_*.html/json             # Drift between train and test data
â”œâ”€â”€ requirements.txt                                    # Python dependencies
â”œâ”€â”€ scripts                                             # Manual scripts to run specific jobs
â”‚   â”œâ”€â”€ load_data_to_postgres.py                        # Script to upload data to PostgreSQL
â”‚   â””â”€â”€ run_drift.py                                    # Script to trigger drift calculation
â”œâ”€â”€ src                                                 # Main source code directory
â”‚   â”œâ”€â”€ airflow                                         # Airflow DAGs and metadata setup
â”‚   â”‚   â”œâ”€â”€ airflow.cfg                                 # Configuration file for Airflow
â”‚   â”‚   â”œâ”€â”€ airflow.db                                  # Metadata DB (SQLite)
â”‚   â”‚   â”œâ”€â”€ dags                                        # All DAG files
â”‚   â”‚   â”‚   â””â”€â”€ drift_retrain_dag.py                    # Main DAG for drift detection + retraining
â”‚   â”‚   â”œâ”€â”€ logs                                        # Airflow log output
â”‚   â”‚   â”œâ”€â”€ scripts                                     # Airflow DAG-triggered scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ check_drift_runner.py                   # Checks data drift
â”‚   â”‚   â”‚   â”œâ”€â”€ retrain_runner.py                       # Triggers retraining process
â”‚   â”‚   â”‚   â””â”€â”€ trigger_upload_monitor.py               # Monitors S3 uploads
â”‚   â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”‚   â””â”€â”€ airflow_loader.py                       # Loads DAG configurations and variables
â”‚   â”‚   â””â”€â”€ webserver_config.py                         # Config for Airflow webserver
â”‚   â”œâ”€â”€ app                                             # Flask app backend
â”‚   â”‚   â”œâ”€â”€ main.py                                     # Main Flask entry point
â”‚   â”‚   â”œâ”€â”€ routes.py                                   # API endpoints for user interaction
â”‚   â”‚   â”œâ”€â”€ templates/index.html                        # Frontend upload interface
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â”œâ”€â”€ prediction.py                           # Code to make predictions
â”‚   â”‚       â””â”€â”€ upload.py                               # Validates and uploads user CSV
â”‚   â”œâ”€â”€ db                                              # Database connection helpers
â”‚   â”‚   â””â”€â”€ db_utils.py                                 # Helper to connect/query PostgreSQL
â”‚   â”œâ”€â”€ drift                                           # Drift detection logic
â”‚   â”‚   â”œâ”€â”€ check_drift.py                              # Drift check between datasets
â”‚   â”‚   â””â”€â”€ test_dirft.py                               # Unit tests for drift module
â”‚   â”œâ”€â”€ eda                                             # Exploratory Data Analysis tools
â”‚   â”‚   â””â”€â”€ profiler.py                                 # Code for generating EDA reports
â”‚   â”œâ”€â”€ logs                                            # Custom logs
â”‚   â””â”€â”€ ml                                              # Machine learning modules
â”‚       â”œâ”€â”€ data_loader/data_loader.py                 # Loads raw or preprocessed data
â”‚       â”œâ”€â”€ evaluation/                                 # Model evaluation and comparison
â”‚       â”‚   â””â”€â”€ metrics.py                              # Accuracy, precision, recall etc.
â”‚       â”œâ”€â”€ model_objects/                              # Saved models for comparison
â”‚       â”œâ”€â”€ pipeline/                                   # Pipeline construction
â”‚       â”‚   â”œâ”€â”€ custom_transformers.py                  # Custom transformers used in pipeline
â”‚       â”‚   â”œâ”€â”€ feature_engineering.py                  # Feature engineering logic
â”‚       â”‚   â”œâ”€â”€ feature_selection.py                    # Feature selection strategies
â”‚       â”‚   â”œâ”€â”€ feature_selector.py                     # Custom transformer to select features
â”‚       â”‚   â”œâ”€â”€ pipeline_runner.py                      # Pipeline runner script
â”‚       â”‚   â”œâ”€â”€ preprocessing.py                        # Preprocessing logic
â”‚       â”‚   â””â”€â”€ schema_validator.py                     # Validates incoming dataframe schema
â”‚       â”œâ”€â”€ registry/model_registry.py                  # Registers and loads models from disk
â”‚       â””â”€â”€ training/                                   # Training logic
â”‚           â”œâ”€â”€ mlflow_logger.py                        # Logs metrics and models to MLflow
â”‚           â”œâ”€â”€ train.py                                # Runs the full model training pipeline
â”‚           â””â”€â”€ train_utils.py                          # Utilities for training
â”œâ”€â”€ structure.txt                                       # Alternate structure file (likely unused)
â”œâ”€â”€ struture.txt                                        # Typo'd duplicate, likely can be removed
â””â”€â”€ uploads                                             # Uploaded CSVs from users
    â”œâ”€â”€ Lead_Scoring.csv                                # Uploaded training data
    â”œâ”€â”€ predicted_data.csv                              # Output file with model predictions
    â”œâ”€â”€ sample_data.csv                                 # Sample format for uploads
    â””â”€â”€ test_Lead_Scoring.csv                           # Uploaded test data
```


# ðŸ›« Example Setup: Airflow with PostgreSQL for Lead Scoring System

> âš ï¸ **NOTE:** All names used here (e.g., database names, project folder paths, user details) are just examples.  
> Please **replace them with your own values** according to your system or project setup.

---

## ðŸ§° Installation Prerequisites
```text
- OS: WSL (Ubuntu) or native Ubuntu system
- Environment: Conda environment (example name: `lead_pipeline_env`)
- Python: Version 3.10 installed
- Project Directory: (example) `~/Projects/lead_pipeline_project/src/airflow/`
```

---

## ðŸ“¦ Step 1 â€“ Install PostgreSQL
```text
sudo apt update
sudo apt install postgresql postgresql-contrib
```

---

## ðŸ› ï¸ Step 2 â€“ Configure PostgreSQL (Example Setup)
```text
# Creates a new PostgreSQL database named 'lead_db_example'
sudo -u postgres psql -c "CREATE DATABASE lead_db_example;"

# (Optional) Enter PostgreSQL CLI for advanced operations
sudo -u postgres psql
```

---

## ðŸ§ª Step 3 â€“ Validate the DB in CLI (Optional)
```text
\l                             # List all available databases
\c lead_db_example            # Connect to the example database
select * from uploaded_leads limit 10;  # Example table query (if table exists)
```

---

## ðŸ Step 4 â€“ Install Airflow in Conda
```text
conda activate lead_pipeline_env  # Replace with your env name

pip install "apache-airflow==2.10.0" \
  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.8.txt"
```

---

## ðŸ—ï¸ Step 5 â€“ Configure Shell Environment
```text
# Open shell config based on your shell
nano ~/.bashrc        # For bash users
nano ~/.zshrc         # For zsh users

# Add these lines (with your paths/envs):
export AIRFLOW_HOME=~/Projects/lead_pipeline_project/src/airflow
conda activate lead_pipeline_env

# Save and reload config
Ctrl + O â†’ Enter â†’ Ctrl + X
source ~/.bashrc
```

---

## ðŸ” Step 6 â€“ Verify Configuration
```text
echo $AIRFLOW_HOME     # Should output your AIRFLOW project directory
which airflow          # Should show path inside your conda env

# (Optional) Remove default Airflow directory
rm -rf ~/airflow
```

---

## ðŸ—ƒï¸ Step 7 â€“ Initialize Airflow DB
```text
airflow db init
```

---

## ðŸ‘¤ Step 8 â€“ Create Airflow Admin User
```text
# Replace user details with your own
airflow users create \
    --username admin \
    --firstname John \
    --lastname Doe \
    --role Admin \
    --email john.doe@example.com
```

---

## ðŸš€ Step 9 â€“ Run Airflow Services
```text
# Terminal 1
airflow webserver --port 8080

# Terminal 2
airflow scheduler
```

---

## ðŸŒ Step 10 â€“ Open Airflow UI
```text
Go to: http://localhost:8080

Login using:
  Username: admin
  Password: (as set during user creation)
```

---

## ðŸ“ Example Project Structure
```text
src/airflow/
â”œâ”€â”€ airflow.cfg         # Configuration file
â”œâ”€â”€ airflow.db          # Metadata DB (SQLite for dev only)
â”œâ”€â”€ dags/               # DAGs go here
â””â”€â”€ logs/               # Execution logs by date and task
```

---

## âš™ï¸ Behavior on Startup
```text
- The AIRFLOW_HOME variable is auto-loaded via shell
- Your Conda environment (example: `lead_pipeline_env`) activates automatically
- All Airflow files are kept inside `src/airflow/` for clean organization
```

---

âœ… **Use this setup for example workflows like:**
- Lead scoring
- Data ingestion
- Model retraining
- Drift detection scheduling

> Let me know if you want example DAGs, PostgreSQL table schemas, or ML model integration!
